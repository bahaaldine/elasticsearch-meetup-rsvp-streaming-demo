[2015-09-01 14:17:09,444][INFO ][node                     ] [lambda-architecture] version[1.6.2], pid[8], build[6220391/2015-07-29T09:24:47Z]
[2015-09-01 14:17:09,446][INFO ][node                     ] [lambda-architecture] initializing ...
[2015-09-01 14:17:09,753][INFO ][plugins                  ] [lambda-architecture] loaded [watcher, license, shield, marvel], sites [marvel]
[2015-09-01 14:17:09,950][INFO ][env                      ] [lambda-architecture] using [1] data paths, mounts [[/data (none)]], net usable_space [15.9gb], net total_space [465.1gb], types [vboxsf]
[2015-09-01 14:17:10,000][INFO ][watcher.trigger.schedule ] [lambda-architecture] using [ticker] schedule trigger engine
[2015-09-01 14:17:11,939][INFO ][transport                ] [lambda-architecture] Using [org.elasticsearch.shield.transport.ShieldServerTransportService] as transport service, overridden by [shield]
[2015-09-01 14:17:11,940][INFO ][transport                ] [lambda-architecture] Using [org.elasticsearch.shield.transport.netty.ShieldNettyTransport] as transport, overridden by [shield]
[2015-09-01 14:17:11,941][INFO ][http                     ] [lambda-architecture] Using [org.elasticsearch.shield.transport.netty.ShieldNettyHttpServerTransport] as http transport, overridden by [shield]
[2015-09-01 14:17:19,287][INFO ][node                     ] [lambda-architecture] initialized
[2015-09-01 14:17:19,294][INFO ][node                     ] [lambda-architecture] starting ...
[2015-09-01 14:17:19,502][INFO ][watcher.actions.email.service] [lambda-architecture] default account set to [work]
[2015-09-01 14:17:21,025][INFO ][shield.transport         ] [lambda-architecture] bound_address {inet[/0:0:0:0:0:0:0:0:9300]}, publish_address {inet[/172.17.0.123:9300]}
[2015-09-01 14:17:21,075][INFO ][discovery                ] [lambda-architecture] logging/b0cUh2gBSyelUJOWaPzy1w
[2015-09-01 14:17:24,891][INFO ][cluster.service          ] [lambda-architecture] new_master [lambda-architecture][b0cUh2gBSyelUJOWaPzy1w][2dfacdfac268][inet[/172.17.0.123:9300]], reason: zen-disco-join (elected_as_master)
[2015-09-01 14:17:24,948][INFO ][watcher.actions.email.service] [lambda-architecture] default account set to [work]
[2015-09-01 14:17:24,951][INFO ][watcher.actions.email.service] [lambda-architecture] default account set to [work]
[2015-09-01 14:17:24,953][INFO ][watcher                  ] [lambda-architecture] starting watch service...
[2015-09-01 14:17:25,212][INFO ][http                     ] [lambda-architecture] bound_address {inet[/0:0:0:0:0:0:0:0:9200]}, publish_address {inet[/172.17.0.123:9200]}
[2015-09-01 14:17:25,213][INFO ][node                     ] [lambda-architecture] started
[2015-09-01 14:17:25,281][INFO ][gateway                  ] [lambda-architecture] recovered [0] indices into cluster_state
[2015-09-01 14:17:25,289][INFO ][license.plugin.core      ] [lambda-architecture] license for [watcher] - valid
[2015-09-01 14:17:25,304][ERROR][watcher.license          ] [lambda-architecture] 
#
# Watcher license will expire on [Thursday, October 01, 2015]. All configured actions on
# all registered watches are throttled (not executed) on Watcher license expiration. 
# Watches will continue be evaluated and watch history will continue being recorded.
# Have a new license? please update it. Otherwise, please reach out to your support contact.
#
[2015-09-01 14:17:25,567][INFO ][shield.license           ] [lambda-architecture] enabling license for [shield]
[2015-09-01 14:17:25,581][INFO ][license.plugin.core      ] [lambda-architecture] license for [shield] - valid
[2015-09-01 14:17:25,582][ERROR][shield.license           ] [lambda-architecture] 
#
# Shield license will expire on [Thursday, October 01, 2015]. Cluster health, cluster stats and indices stats operations are
# blocked on Shield license expiration. All data operations (read and write) continue to work. If you
# have a new license, please update it. Otherwise, please reach out to your support contact.
#
[2015-09-01 14:17:25,797][INFO ][watcher                  ] [lambda-architecture] watch service has started
[2015-09-01 14:17:26,873][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},dc=elastic,dc=co] and DN [cn=server,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:26,895][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},ou=Users,dc=elastic,dc=co] and DN [cn=server,ou=Users,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:26,909][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},ou=Marvels,dc=elastic,dc=co] and DN [cn=server,ou=Marvels,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:26,911][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},ou=Watchers,dc=elastic,dc=co] and DN [cn=server,ou=Watchers,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:29,443][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},dc=elastic,dc=co] and DN [cn=agent,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:29,451][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},ou=Users,dc=elastic,dc=co] and DN [cn=agent,ou=Users,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:30,436][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] creating index, cause [auto(bulk api)], templates [marvel, template.json], shards [1]/[1], mappings [_default_, shard_event, index_event, index_stats, node_event, routing_event, cluster_event, cluster_state, cluster_stats, node_stats, indices_stats]
[2015-09-01 14:17:31,412][WARN ][shield.authc.ldap        ] [lambda-architecture] failed LDAP authentication with user template [cn={0},dc=elastic,dc=co] and DN [cn=bahaaldine,dc=elastic,dc=co]: invalid credentials
[2015-09-01 14:17:31,923][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [node_stats] (dynamic)
[2015-09-01 14:17:32,086][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [cluster_event] (dynamic)
[2015-09-01 14:17:32,122][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [index_event] (dynamic)
[2015-09-01 14:17:32,231][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [routing_event] (dynamic)
[2015-09-01 14:17:32,248][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [node_event] (dynamic)
[2015-09-01 14:17:32,261][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [cluster_state] (dynamic)
[2015-09-01 14:17:32,439][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [indices_stats] (dynamic)
[2015-09-01 14:17:32,448][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [index_stats] (dynamic)
[2015-09-01 14:17:32,621][INFO ][cluster.metadata         ] [lambda-architecture] [.marvel-2015.09.01] update_mapping [cluster_stats] (dynamic)
[2015-09-01 14:17:33,795][DEBUG][action.admin.cluster.health] [lambda-architecture] observer: timeout notification from cluster service. timeout setting [5s], time since start [5s]
[2015-09-01 14:17:54,937][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 15.9gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:17:54,938][INFO ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-09-01 14:18:06,769][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] creating index, cause [auto(bulk api)], templates [template.json], shards [1]/[0], mappings [_default_, rsvp]
[2015-09-01 14:18:07,016][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] update_mapping [rsvp] (dynamic)
[2015-09-01 14:18:08,035][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] update_mapping [rsvp] (dynamic)
[2015-09-01 14:18:09,102][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] update_mapping [rsvp] (dynamic)
[2015-09-01 14:18:24,941][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 15.9gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:18:29,677][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] update_mapping [rsvp] (dynamic)
[2015-09-01 14:18:51,226][INFO ][cluster.metadata         ] [lambda-architecture] [.kibana] creating index, cause [api], templates [template.json], shards [1]/[1], mappings [_default_]
[2015-09-01 14:18:51,572][INFO ][cluster.metadata         ] [lambda-architecture] [.kibana] update_mapping [config] (dynamic)
[2015-09-01 14:18:54,939][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:18:54,939][INFO ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-09-01 14:18:56,132][INFO ][cluster.metadata         ] [lambda-architecture] [.kibana] create_mapping [index-pattern]
[2015-09-01 14:18:57,117][INFO ][cluster.metadata         ] [lambda-architecture] [.kibana] update_mapping [config] (dynamic)
[2015-09-01 14:19:08,036][INFO ][cluster.metadata         ] [lambda-architecture] [.kibana] create_mapping [visualization]
[2015-09-01 14:19:24,939][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:19:54,939][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:19:54,940][INFO ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-09-01 14:20:24,953][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:20:43,078][INFO ][cluster.metadata         ] [lambda-architecture] [meetup-2015] update_mapping [rsvp] (dynamic)
[2015-09-01 14:20:54,941][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:20:54,941][INFO ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-09-01 14:21:24,944][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:21:54,946][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
[2015-09-01 14:21:54,946][INFO ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark exceeded on one or more nodes, rerouting shards
[2015-09-01 14:22:24,948][WARN ][cluster.routing.allocation.decider] [lambda-architecture] high disk watermark [90%] exceeded on [b0cUh2gBSyelUJOWaPzy1w][lambda-architecture] free: 16gb[3.4%], shards will be relocated away from this node
